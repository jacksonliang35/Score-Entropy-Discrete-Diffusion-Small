ngpus: 1
# tokens: 256

training:
  batch_size: 64
  accum: 1
  n_iters: 1000000
  n_epochs: 1000
  snapshot_freq: 10000
  log_freq: 50
  eval_freq: 100
  weight: standard
  snapshot_sampling: True
  ema: 0.9999

model:
  hidden_size: 512
  cond_dim: 128
  length: 32
  n_blocks: 8
  n_heads: 8
  dropout: 0.1

# data:
#   train: openwebtext
#   valid: wikitext103
#   cache_dir: data

graph:
  type: uniform
  file: data
  report_all: False

noise:
  type: geometric
  sigma_min: 1e-4
  sigma_max: 20

sampling:
  predictor: euler
  steps: 128
  noise_removal: True

eval:
  batch_size: 16
  perplexity: True
  perplexity_batch_size: 4

optim:
  weight_decay: 0
  optimizer: AdamW
  lr: 1e-3
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8
  warmup: 2500
  grad_clip: 1.
